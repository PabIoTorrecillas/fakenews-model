{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c71438",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NOTEBOOK: Evaluaci√≥n Completa del Modelo BERT\n",
    "# ============================================\n",
    "# Guarda este c√≥digo como: notebooks/05_evaluate_bert_model.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ============================================\n",
    "# 1. CARGAR MODELO ENTRENADO\n",
    "# ============================================\n",
    "\n",
    "MODEL_PATH = './models/bert_fakenews_v1'\n",
    "\n",
    "print(\"üîÑ Cargando modelo...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. CARGAR DATOS DE TEST\n",
    "# ============================================\n",
    "\n",
    "test_df = pd.read_csv('data/final/test_final.csv')\n",
    "print(f\"\\nüìä Test set: {len(test_df)} samples\")\n",
    "\n",
    "# ============================================\n",
    "# 3. FUNCI√ìN DE PREDICCI√ìN\n",
    "# ============================================\n",
    "\n",
    "def predict_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Predice en batch para eficiencia\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "# ============================================\n",
    "# 4. HACER PREDICCIONES EN TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüîÑ Haciendo predicciones en test set...\")\n",
    "y_true = test_df['label'].values\n",
    "texts = test_df['text'].tolist()\n",
    "\n",
    "y_pred, y_probs = predict_batch(texts)\n",
    "\n",
    "print(\"‚úÖ Predicciones completadas\")\n",
    "\n",
    "# ============================================\n",
    "# 5. M√âTRICAS GENERALES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà M√âTRICAS GENERALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average='binary'\n",
    ")\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(\n",
    "    y_true, y_pred, \n",
    "    target_names=['Fake', 'Real'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# ============================================\n",
    "# 6. CONFUSION MATRIX (Visualizaci√≥n)\n",
    "# ============================================\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=['Fake', 'Real'],\n",
    "    yticklabels=['Fake', 'Real']\n",
    ")\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Confusion matrix guardada en: results/confusion_matrix.png\")\n",
    "\n",
    "# ============================================\n",
    "# 7. ROC CURVE\n",
    "# ============================================\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ ROC curve guardada en: results/roc_curve.png\")\n",
    "print(f\"üìä AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 8. DISTRIBUCI√ìN DE PROBABILIDADES\n",
    "# ============================================\n",
    "\n",
    "confidence_fake = y_probs[y_true == 0][:, 0]  # Confianza cuando es Fake\n",
    "confidence_real = y_probs[y_true == 1][:, 1]  # Confianza cuando es Real\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence_fake, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "plt.title('Distribuci√≥n de Confianza - Fake News', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Probabilidad', fontsize=11)\n",
    "plt.ylabel('Frecuencia', fontsize=11)\n",
    "plt.axvline(confidence_fake.mean(), color='darkred', linestyle='--', \n",
    "            linewidth=2, label=f'Media: {confidence_fake.mean():.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(confidence_real, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Distribuci√≥n de Confianza - Real News', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Probabilidad', fontsize=11)\n",
    "plt.ylabel('Frecuencia', fontsize=11)\n",
    "plt.axvline(confidence_real.mean(), color='darkgreen', linestyle='--', \n",
    "            linewidth=2, label=f'Media: {confidence_real.mean():.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Distribuci√≥n de confianza guardada en: results/confidence_distribution.png\")\n",
    "\n",
    "# ============================================\n",
    "# 9. AN√ÅLISIS DE ERRORES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç AN√ÅLISIS DE ERRORES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Falsos Positivos (predijo Real, pero era Fake)\n",
    "fp_indices = np.where((y_pred == 1) & (y_true == 0))[0]\n",
    "print(f\"\\n‚ùå Falsos Positivos: {len(fp_indices)}\")\n",
    "\n",
    "if len(fp_indices) > 0:\n",
    "    print(\"\\nEjemplos de Falsos Positivos (predijo Real cuando era Fake):\")\n",
    "    for i in fp_indices[:3]:\n",
    "        print(f\"\\n{i+1}. Confianza: {y_probs[i][1]*100:.1f}%\")\n",
    "        print(f\"   Texto: {texts[i][:200]}...\")\n",
    "\n",
    "# Falsos Negativos (predijo Fake, pero era Real)\n",
    "fn_indices = np.where((y_pred == 0) & (y_true == 1))[0]\n",
    "print(f\"\\n‚ùå Falsos Negativos: {len(fn_indices)}\")\n",
    "\n",
    "if len(fn_indices) > 0:\n",
    "    print(\"\\nEjemplos de Falsos Negativos (predijo Fake cuando era Real):\")\n",
    "    for i in fn_indices[:3]:\n",
    "        print(f\"\\n{i+1}. Confianza: {y_probs[i][0]*100:.1f}%\")\n",
    "        print(f\"   Texto: {texts[i][:200]}...\")\n",
    "\n",
    "# ============================================\n",
    "# 10. AN√ÅLISIS POR LONGITUD DE TEXTO\n",
    "# ============================================\n",
    "\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\n",
    "test_df['prediction'] = y_pred\n",
    "test_df['correct'] = (test_df['label'] == test_df['prediction']).astype(int)\n",
    "\n",
    "# Crear bins de longitud\n",
    "test_df['length_bin'] = pd.cut(\n",
    "    test_df['word_count'], \n",
    "    bins=[0, 100, 200, 300, 500, 1000],\n",
    "    labels=['0-100', '100-200', '200-300', '300-500', '500+']\n",
    ")\n",
    "\n",
    "accuracy_by_length = test_df.groupby('length_bin')['correct'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_by_length.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Accuracy por Longitud de Texto', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Rango de Palabras', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/accuracy_by_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Accuracy por longitud de texto:\")\n",
    "print(accuracy_by_length)\n",
    "\n",
    "# ============================================\n",
    "# 11. AN√ÅLISIS POR FUENTE\n",
    "# ============================================\n",
    "\n",
    "if 'source' in test_df.columns:\n",
    "    accuracy_by_source = test_df.groupby('source')['correct'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    accuracy_by_source.plot(kind='bar', color='coral', edgecolor='black')\n",
    "    plt.title('Accuracy por Fuente de Datos', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Fuente', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylim([0.8, 1.0])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/accuracy_by_source.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Accuracy por fuente:\")\n",
    "    print(accuracy_by_source)\n",
    "\n",
    "# ============================================\n",
    "# 12. PRUEBAS CON EJEMPLOS PERSONALIZADOS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ PRUEBAS CON EJEMPLOS PERSONALIZADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_examples = [\n",
    "    \"Breaking: Scientists discover cure for cancer in revolutionary study published today.\",\n",
    "    \"You won't believe what this celebrity did! Doctors hate this one weird trick!\",\n",
    "    \"The Federal Reserve announced interest rate changes during today's meeting.\",\n",
    "    \"Aliens confirmed to have visited Earth, says anonymous government source.\",\n",
    "    \"New research from MIT shows promising results in renewable energy technology.\"\n",
    "]\n",
    "\n",
    "def predict_single(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        conf = probs[0][pred].item()\n",
    "    \n",
    "    return pred, conf, probs[0].cpu().numpy()\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    pred, conf, probs = predict_single(example)\n",
    "    label = \"Real\" if pred == 1 else \"Fake\"\n",
    "    \n",
    "    print(f\"\\n{i}. Texto: {example}\")\n",
    "    print(f\"   Predicci√≥n: {label}\")\n",
    "    print(f\"   Confianza: {conf*100:.2f}%\")\n",
    "    print(f\"   Prob(Fake)={probs[0]*100:.1f}% | Prob(Real)={probs[1]*100:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# 13. GUARDAR REPORTE FINAL\n",
    "# ============================================\n",
    "\n",
    "report = {\n",
    "    'model_path': MODEL_PATH,\n",
    "    'test_samples': len(test_df),\n",
    "    'metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'auc': float(roc_auc)\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'true_negative': int(cm[0][0]),\n",
    "        'false_positive': int(cm[0][1]),\n",
    "        'false_negative': int(cm[1][0]),\n",
    "        'true_positive': int(cm[1][1])\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'false_positives': len(fp_indices),\n",
    "        'false_negatives': len(fn_indices)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('results/evaluation_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ EVALUACI√ìN COMPLETADA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Archivos generados:\")\n",
    "print(f\"   - results/confusion_matrix.png\")\n",
    "print(f\"   - results/roc_curve.png\")\n",
    "print(f\"   - results/confidence_distribution.png\")\n",
    "print(f\"   - results/accuracy_by_length.png\")\n",
    "print(f\"   - results/evaluation_report.json\")\n",
    "print(\"\\nüéâ ¬°An√°lisis completo!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
